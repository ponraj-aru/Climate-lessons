{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excerise 1: Extracting station information from downloaded station netcdf files \n",
    "#### Meta data Temperature - gridded daily mean temperature in the Netherlands\n",
    "Gridded files of daily mean temperature in the Netherlands. Based on 33 -35 automatic weather stations of the KNMI (https://dataplatform.knmi.nl/dataset/tg1-5).The interpolation method is Inverse Distance Weighted interpolation (IDW) using power parameter 2.0. Block size is 20km and search radius 110km. Due to use of a block it is not an exact interpolator: the interpolated value at a point can differ from the measured value. The number of observations changes from 15 (1961) to 26 (1991), 37 (1994) and 34 (2012). Version 5 has an updated extent.\n",
    "\n",
    "More information avaiable from \thttp://www.knmi.nl/bibliotheek/stageverslagen/stageverslag_Salet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import glob\n",
    "from scipy.interpolate import griddata\n",
    "#import rasterio\n",
    "#from rasterio.features import geometry_mask\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from rasterio.transform import from_origin\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import random\n",
    "import pickletools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide your inputs for data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\arumu002\\\\OneDrive - Wageningen University & Research\\\\Project I\\\\Rutger\"\n",
    "start_year = 1991 # choose the starting of climatological year\n",
    "end_year = 2021 # choose the end of climatological year\n",
    "station_list = data_path+\"\\\\KNMI_weather_stations.csv\" # Writing station after extracting from the netcdf files\n",
    "station_list_pkl = data_path+\"\\\\df_stations.pkl\" # Writing station as pickle file after extracting from the netcdf files\n",
    "ext_climate = data_path+\"\\\\KNMI_weather_stations_data.csv\" # Writing extracted weather information as csv\n",
    "ext_climate_pkl = data_path+\"\\\\KNMI_weather_stations_data.pkl\" # Writing extracted weather information as pkl\n",
    "shp1 = data_path+'\\\\shp\\\\NET_admin_0.shp' # Netherlands country level shapefile\n",
    "shp2 = data_path+'\\\\shp\\\\NET_admin_1.shp' # Netherlands state level shapefile\n",
    "shp3 = data_path+'\\\\shp\\\\agriculture_areas2.shp'# Nethelrands land based region shapefile\n",
    "shp4 = data_path+'\\\\shp\\\\+station_points.shp' # write the station as shapfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART I - Extract list of stations avaiable from gridded data from 1990 to 2021\n",
    "##### The list of stations over the years keep varying. So, this script extract the common stations based on coordinates over the current climatology period (1990-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(station_list):\n",
    "    print(\"List of common stations avaiable from the years 1990 to 2022 has been extracted. Please proceed to further steps.\")\n",
    "    df_stations = pd.read_csv(station_list)\n",
    "    print(\"Overall, \"+ str(len(df_stations))+\" stations avaiable from 1990-2021\")\n",
    "else:\n",
    "    print(\"Stations will be extracted from the years 1990-2022. Please wait until the process done\")\n",
    "    dataframes_list = []\n",
    "    for i in range(start_year,end_year):\n",
    "        file_paths = glob.glob(data_path + \"\\\\\" + str(i) + '\\\\*.nc')\n",
    "        print(\"Merging daily files for the year: \"+ str(i))\n",
    "        ds = xr.open_mfdataset(file_paths, combine='by_coords')\n",
    "        stations = ds['stations'].to_dataframe()\n",
    "        stations1 = stations.dropna()\n",
    "        list_stations = stations1.drop_duplicates() \n",
    "        list_stations.reset_index(drop=True, inplace=True)\n",
    "        list_stations_f = list_stations.drop('stations', axis=1)\n",
    "        dataframes_list.append(list_stations_f)\n",
    "        print(\"Stations are getting extracted for the year: \"+str(i)+\" and \" +str(len(list_stations_f))+ \" stations found.\")\n",
    "# Initialize common_df with the first non-empty DataFrame\n",
    "    df_stations = None\n",
    "# Loop through the DataFrames\n",
    "    for df in dataframes_list:\n",
    "        if df_stations is None:\n",
    "            df_stations = df\n",
    "        elif not df.empty:\n",
    "        # Use 'lon' and 'lat' columns for merging\n",
    "            common_df = pd.merge(df_stations, df, on=['lon', 'lat'], how='inner')\n",
    "            df_stations = common_df.drop_duplicates()\n",
    "            df_stations.reset_index(drop=True, inplace=True)\n",
    "            df_stations['ID'] = df_stations.index.map(lambda x: 'ID_{:03}'.format(x + 1))\n",
    "# Display the common DataFrame\n",
    "    if df_stations is not None:\n",
    "        print(\"Overall, over the \"+str(len(range(start_year,end_year)))+\" years \"+ str(len(df_stations)) +\" stations found in common\")\n",
    "    else:\n",
    "        print(\"No common DataFrame found.\")\n",
    "    df_stations.to_csv(station_list,index=False)\n",
    " # extract stations from all years and add into the list\n",
    "filename = station_list_pkl\n",
    "# Open the file in binary write mode and save the list of DataFrames using pickle\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(df_stations, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART II - Extracting climate information for avaiable stations from gridded data from 1990 to 2021\n",
    "##### This script extract climate information for the years given and write as a database (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ext_climate):\n",
    "    print(\"Extracted climate information for the stataions from 1990 to 2021. Please proceed to next step ...\")\n",
    "else:\n",
    "    print(\"Extracting climate information for the extraction loactaions from 1990 to 2021 has started. Please wait...\")\n",
    "    with open(station_list_pkl, 'rb') as file:\n",
    "        df_stations = pickle.load(file)\n",
    "    df_all = []\n",
    "    for i in range(start_year,end_year):\n",
    "        file_paths = glob.glob(data_path + \"\\\\\" + str(i) + '\\\\*.nc')\n",
    "        print(\"Merging daily files for the year: \" + str(i))\n",
    "        ds = xr.open_mfdataset(file_paths, combine='by_coords')\n",
    "        #################################################################################################################\n",
    "        ######################### Extract rainfall for extracted staions netcdf file #######################\n",
    "        stations_vals = ds['stationvalues'].to_dataframe()\n",
    "        stations_vals1 = stations_vals.reset_index(level='time')\n",
    "        stations_vals1.reset_index(drop=True, inplace=True)\n",
    "        #station_filtered = stations_vals1[stations_vals1[['lon', 'lat']].isin(df_stations.to_dict(orient='list')).all(axis=1)]\n",
    "        result_df = pd.merge(stations_vals1, df_stations, on=('lon', 'lat'), how='inner')\n",
    "        #result_df1 = result_df.dropna(subset=['ID'])\n",
    "        result_df['stationvalues'].fillna(0, inplace=True)\n",
    "        print(result_df)\n",
    "        df_wide_pivot = result_df.pivot(index='time', columns='ID', values='stationvalues')\n",
    "        #print(df_wide_pivot)\n",
    "        df_all.append(df_wide_pivot)\n",
    "        #df_wide_pivot.to_csv(str(i) + \".csv\")  # This part writes the raw climate files  \n",
    "    df_f = pd.concat(df_all, ignore_index=False)\n",
    "    #print(df_f)\n",
    "    df_f.to_csv(ext_climate) # This part writes the raw climate files\n",
    "    filename = ext_climate_pkl\n",
    "# Open the file in binary write mode and save the list of DataFrames using pickle\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(df_f, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART III: Plotting the stations\n",
    "##### As the station IDs and names also varys over the years, this script creates a station ID with suffix of 'ID_' (i.e., ID_001, ID 002, etc). Which helps to identify station which need to be anlalyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PLOT THE STATIONS #################\n",
    "# Create a GeoDataFrame with 'geometry' column\n",
    "df_stations = pd.read_csv(data_path+\"\\\\\"+\"KNMI_weather_stations.csv\")\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(df_stations['lon'], df_stations['lat'])]\n",
    "gdf_points = gpd.GeoDataFrame(df_stations, geometry=geometry, crs='EPSG:4326')\n",
    "gdf_points.to_file(data_path+\"\\\\shp\\\\\"+\"station_points.shp\")\n",
    "# Load a world map dataset and filter the Netherlands\n",
    "netherlands_shapefile1 = gpd.read_file(data_path+'\\\\shp\\\\NET_admin_0.shp')\n",
    "netherlands_shapefile2 = gpd.read_file(data_path+'\\\\shp\\\\NET_admin_1.shp')\n",
    "netherlands_shapefile3 = gpd.read_file(data_path+'\\\\shp\\\\agriculture_areas2.shp')\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "netherlands_shapefile2.plot(ax=ax,color='none', edgecolor='lightgray')\n",
    "netherlands_shapefile1.plot(ax=ax, color='none',edgecolor='black')\n",
    "# Plot the points on top of the Netherlands shapefile\n",
    "gdf_points.plot(ax=ax, marker='^', color='red', markersize=30, alpha=0.5)\n",
    "for x, y, label in zip(gdf_points.centroid.x, gdf_points.centroid.y, gdf_points['ID']):\n",
    "    ax.text(x, y, label, fontsize=10, ha='center', va='center')\n",
    "# Set the title and axis labels\n",
    "ax.set_title('KNMI weather stations'+\" (\"+str(len(df_stations))+\" stations)\")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# Set the axis limits to focus on the Netherlands\n",
    "ax.set_xlim([3, 8])\n",
    "ax.set_ylim([50.5, 54])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART IV - Analyzing climate data for the climatology year (i.e., 30 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide your inputs\n",
    "station_id = \"ID_040\"\n",
    "param = \"Rainfall\"\n",
    "season_start=1\n",
    "season_end=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ext_climate_pkl, 'rb') as file:\n",
    "    df_f = pickle.load(file)\n",
    "filtered_df = df_f[df_f.index.month.isin([season_start,season_end])]\n",
    "# Sum the 'Value' column for all years\n",
    "sum_by_year = filtered_df[station_id].groupby(filtered_df.index.year).sum()\n",
    "# Display the sum for each year\n",
    "print(sum_by_year)\n",
    "# Create a time series plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sum_by_year.index, sum_by_year.values, marker='o', linestyle='-')\n",
    "plt.title(param+' over the location: '+station_id+' for the months of '+str(season_start)+' and '+str(season_end))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel(param)\n",
    "plt.grid(True)\n",
    "# Fit a trend line (linear regression) to the data\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(sum_by_year.index, sum_by_year.values)\n",
    "# Print a message based on the slope value\n",
    "if slope < 0:\n",
    "    print(\"Rainfall over the period shows a Decreasing Trend.\")\n",
    "else:\n",
    "    print(\"Rainfall over the period shows an Increasing Trend.\")\n",
    "trendline = slope * np.array(sum_by_year.index) + intercept\n",
    "plt.plot(sum_by_year.index, trendline, color='red', linestyle='--', label='Trend Line')\n",
    "# Display the plot with the trend line\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART V - Analyzing monthly rainfall data for a specific year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide your inputs\n",
    "station_id = \"ID_064\"\n",
    "param = \"Rainfall\"\n",
    "year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '2023' with the desired year\n",
    "with open(ext_climate_pkl, 'rb') as file:\n",
    "    df_f = pickle.load(file)\n",
    "# Sum the 'Value' column for all years\n",
    "# Filter the DataFrame for the selected year\n",
    "result_for_selected_year = df_f[df_f.index.year == year]\n",
    "# Calculate the sum of the 'value' column for each month in the selected year\n",
    "monthly_sum_for_selected_year = result_for_selected_year[station_id].groupby(result_for_selected_year.index.month).sum()\n",
    "print(monthly_sum_for_selected_year)\n",
    "# Define the month labels\n",
    "month_labels = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "monthly_sum_for_selected_year.plot(kind='bar')\n",
    "plt.xticks(range(12), month_labels)  # Replace the default month labels with shortened labels\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Rainfall\")\n",
    "# Calculate x-coordinates for the line plot\n",
    "x = np.arange(len(monthly_sum_for_selected_year))\n",
    "# Add a line connecting the tops of the bars\n",
    "plt.plot(x, monthly_sum_for_selected_year.values, marker='^', linestyle='--', color='red')\n",
    "plt.title(f\"Monthly Sum for {year} for the station ID: \"+station_id)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
